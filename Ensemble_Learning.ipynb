{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it\n",
        "\n",
        "Ans- Ensemble Learning is the technique of combining multiple machine learning models to create a more accurate predictor.\n",
        "\n",
        "Key Idea: The \"Wisdom of the Crowd\"â€”aggregating multiple \"weak\" models to cancel out individual errors and produce one \"strong\" model.\n",
        "\n",
        "Bagging: Trains models in parallel on random data subsets to reduce variance (e.g., Random Forest).\n",
        "\n",
        "Boosting: Trains models sequentially, where each new model fixes errors from the previous one to reduce bias (e.g., XGBoost).\n",
        "\n",
        "Stacking: Uses a \"meta-model\" to learn the best way to combine predictions from different types of models."
      ],
      "metadata": {
        "id": "Mo93KKnRYbtC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between Bagging and Boosting\n",
        "\n",
        "Ans- Training Process: Bagging trains models in parallel (independently), while Boosting trains models sequentially (each depends on the previous one).\n",
        "\n",
        "Data Selection: Bagging uses random subsets (Bootstrap); Boosting weighs misclassified data points more heavily in subsequent rounds.\n",
        "\n",
        "Goal: Bagging aims to reduce variance (overfitting); Boosting aims to reduce bias (underfitting).\n",
        "\n",
        "Weighting: In Bagging, all models have equal weight in the final vote. In Boosting, models with higher accuracy are given more influence."
      ],
      "metadata": {
        "id": "Eqk4RPMbYrYh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest\n",
        "\n",
        "Ans- Bootstrap Sampling is a statistical technique where multiple random subsets are created from a dataset by sampling with replacement.\n",
        "\n",
        "Process: Each subset is the same size as the original, but some rows appear multiple times while others (roughly 37%) are left out.\n",
        "\n",
        "Role in Bagging: It ensures each model in the ensemble (like a Decision Tree) sees a slightly different version of the data.\n",
        "\n",
        "Effect: This diversity prevents the models from making the same errors, which significantly reduces variance and prevents overfitting.\n",
        "\n",
        "Random Forest: In addition to bootstrap sampling, Random Forest also selects a random subset of features for each split to further increase model independence."
      ],
      "metadata": {
        "id": "4BvNXKgEYyYx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models\n",
        "\n",
        "Ans- Out-of-Bag (OOB) samples are the data points left out of the training set during the bootstrap sampling process.\n",
        "\n",
        "**Key Characteristics**\n",
        "\n",
        "Origin: When sampling with replacement, roughly 36.8% of the original data is not selected for a specific base model.\n",
        "\n",
        "Role: These samples act as a \"built-in\" test set for that specific model since it never saw them during training.\n",
        "\n",
        "**How OOB Score is Used**\n",
        "\n",
        "Evaluation: The OOB Score is the average accuracy (or error) calculated by testing each base model only on its corresponding OOB samples.\n",
        "\n",
        "Validation Alternative: It provides a reliable estimate of the model's generalization performance without needing a separate validation set or cross-validation.\n",
        "\n",
        "Efficiency: It allows you to use the entire dataset for training while still obtaining a rigorous performance metric."
      ],
      "metadata": {
        "id": "ZOmysmR1Y6Tw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest\n",
        "\n",
        "Ans- Stability: Decision Trees are highly sensitive to small data changes, causing feature rankings to shift. Random Forests provide stable rankings by averaging importance across hundreds of trees.\n",
        "\n",
        "Calculation Method: Trees calculate importance based on a single set of splits. Random Forests average the Mean Decrease in Impurity (MDI) across the entire ensemble.\n",
        "\n",
        "Handling Correlation: A Decision Tree often picks one feature and ignores others that are highly correlated. A Random Forest distributes importance across correlated features due to random feature selection at each node.\n",
        "\n",
        "Bias: Single trees are biased toward features with high cardinality (many unique values). Random Forests mitigate this by using different data subsets (bagging) and feature subsets.\n",
        "\n",
        "Reliability: Random Forest importance is generally considered more reliable for feature selection because it captures a broader range of patterns than a single \"greedy\" tree."
      ],
      "metadata": {
        "id": "arpVatLAZDtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "\n",
        "Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "\n",
        "Train a Random Forest Classifier\n",
        "\n",
        "Print the top 5 most important features based on feature importance scores."
      ],
      "metadata": {
        "id": "uNlQyuZCZRI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Load data\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# 2. Train Random Forest\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "model.fit(X, y)\n",
        "\n",
        "# 3. Extract and print top 5 features\n",
        "importances = pd.Series(model.feature_importances_, index=feature_names)\n",
        "top_5 = importances.sort_values(ascending=False).head(5)\n",
        "\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KYVZqbdpZYBJ",
        "outputId": "08175b41-a118-47a2-e636-aa6ca739a925"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "worst area              0.139357\n",
            "worst concave points    0.132225\n",
            "mean concave points     0.107046\n",
            "worst radius            0.082848\n",
            "worst perimeter         0.080850\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "\n",
        "Evaluate its accuracy and compare with a single Decision Tree"
      ],
      "metadata": {
        "id": "NEB7O32sZfdJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load data\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Single Decision Tree\n",
        "tree = DecisionTreeClassifier(random_state=42)\n",
        "tree.fit(X_train, y_train)\n",
        "tree_acc = accuracy_score(y_test, tree.predict(X_test))\n",
        "\n",
        "# 3. Bagging Classifier (Ensemble of 50 trees)\n",
        "bagging = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bagging.fit(X_train, y_train)\n",
        "bagging_acc = accuracy_score(y_test, bagging.predict(X_test))\n",
        "\n",
        "# 4. Compare Results\n",
        "print(f\"Single Decision Tree Accuracy: {tree_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy:   {bagging_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RT839cLzZkoR",
        "outputId": "5b113c3f-f1ff-4e2a-9137-d4276e6d25d4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0000\n",
            "Bagging Classifier Accuracy:   1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "Train a Random Forest Classifier\n",
        "\n",
        "Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "\n",
        "Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "mXowReJdZnjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load data\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Setup GridSearchCV\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 3. Evaluate Results\n",
        "best_params = grid_search.best_params_\n",
        "accuracy = accuracy_score(y_test, grid_search.best_estimator_.predict(X_test))\n",
        "\n",
        "print(f\"Best Parameters: {best_params}\")\n",
        "print(f\"Final Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqgd7CiAZsNq",
        "outputId": "ddae7a97-8d3d-40cb-dffd-c0374d0d4e36"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 200}\n",
            "Final Accuracy: 0.9649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Write a Python program to:\n",
        "\n",
        "Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "\n",
        "Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "9iHiP5noZ6pZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load data\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data[:2000], data.target[:2000] # Subset for speed\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train Bagging Regressor\n",
        "bagging = BaggingRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
        "bag_mse = mean_squared_error(y_test, bagging.predict(X_test))\n",
        "\n",
        "# 3. Train Random Forest Regressor\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
        "rf_mse = mean_squared_error(y_test, rf.predict(X_test))\n",
        "\n",
        "# 4. Compare\n",
        "print(f\"Bagging Regressor MSE:    {bag_mse:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGlfUHnOaB35",
        "outputId": "84ae9dd7-edf4-45c0-b833-8f7aebd37898"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE:    0.1485\n",
            "Random Forest Regressor MSE: 0.1500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "Choose between Bagging or Boosting\n",
        "\n",
        "Handle overfitting\n",
        "\n",
        "Select base models\n",
        "\n",
        "Evaluate performance using cross-validation\n",
        "\n",
        "Justify how ensemble learning improves decision-making in this real-world\n",
        "context."
      ],
      "metadata": {
        "id": "Hp_3H6fWaFZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans- **1. Choosing Between Bagging and Boosting**\n",
        "\n",
        "Decision: Boosting (e.g., XGBoost or LightGBM) is generally preferred for financial tabular data.\n",
        "\n",
        "Reason: It focuses on misclassified \"difficult\" cases (customers on the edge of defaulting), minimizing bias and leading to higher accuracy in risk assessment.\n",
        "\n",
        "**2. Handling Overfitting**\n",
        "\n",
        "Early Stopping: Halt training when performance on a validation set stops improving.\n",
        "\n",
        "Regularization: Use 3$L1$ and 4$L2$ penalties to keep model weights small.\n",
        "\n",
        "Subsampling: Train on random subsets of rows and columns (features) to prevent the model from memorizing noise.\n",
        "\n",
        "**3. Selecting Base Models**\n",
        "\n",
        "Diversity: Use a mix of Decision Trees (for non-linear patterns) and Logistic Regression (for stable linear trends).\n",
        "\n",
        "Weak Learners: Use shallow trees (low max_depth) as base models in Boosting to ensure the ensemble remains robust and generalizes well.\n",
        "\n",
        "**4. Evaluating Performance with Cross-Validation**\n",
        "\n",
        "Stratified K-Fold: Use 5 or 10 folds while maintaining the default/non-default ratio in each fold to handle class imbalance.\n",
        "\n",
        "Metrics: Prioritize ROC-AUC or Precision-Recall AUC over accuracy, as correctly identifying a high-risk defaulter is more critical than overall correctness.\n",
        "\n",
        "**5. Justifying Ensemble Learning in Finance**\n",
        "\n",
        "Risk Mitigation: By averaging multiple viewpoints, the institution reduces the \"individual error\" of a single model, leading to fewer bad loans.\n",
        "\n",
        "Improved Explainability: Ensembles provide stable Feature Importance rankings, helping regulators understand why a loan was denied (e.g., debt-to-income ratio vs. credit age).\n",
        "\n",
        "Reliability: The \"Wisdom of the Crowd\" ensures the system is less sensitive to sudden market shifts or outliers in transaction history."
      ],
      "metadata": {
        "id": "S7ghS3hyaPyp"
      }
    }
  ]
}