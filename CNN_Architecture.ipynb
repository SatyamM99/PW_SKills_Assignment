{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN)?\n",
        "\n",
        "Ans- In a Convolutional Neural Network (CNN), Filters and Feature Maps work together to extract and process spatial patterns from data (like images).\n",
        "\n",
        "**Role of Filters (Kernels)**\n",
        "\n",
        "Feature Detectors: Small weight matrices (e.g., $3\\times3$) that slide over the input to identify specific patterns like edges, textures, or shapes.\n",
        "\n",
        "Local Connectivity: They focus on small local regions, allowing the network to understand spatial relationships.\n",
        "\n",
        "Weight Sharing: The same filter is used across the whole image, reducing the number of parameters and making the model efficient.\n",
        "\n",
        "**Role of Feature Maps**\n",
        "\n",
        "Output Signal: The result of applying a filter; it highlights where specific features were detected in the input.\n",
        "\n",
        "Spatial Mapping: High values in a feature map indicate the strong presence of a specific pattern at that location.\n",
        "\n",
        "Abstraction: As data goes deeper into the network, feature maps move from simple edges to complex object parts.\n",
        "\n",
        "**Key Interaction**\n",
        "\n",
        "1:1 Relationship: Each unique filter produces exactly one corresponding feature map.\n",
        "\n",
        "Non-Linearity: Feature maps are typically passed through a ReLU activation function to help the network learn complex patterns."
      ],
      "metadata": {
        "id": "SXfk4j_nPsjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps?\n",
        "\n",
        "Ans- **Padding**\n",
        "\n",
        "Definition: Adding extra pixels (usually zeros) around the border of the input image before applying convolution.\n",
        "\n",
        "Purpose: Prevents information loss at the edges and controls the spatial size of the output.\n",
        "\n",
        "Types: * Valid: No padding; output is smaller than input.\n",
        "\n",
        "Same: Padding added so output size matches input size.\n",
        "\n",
        "**Stride**\n",
        "\n",
        "Definition: The number of pixels by which the filter shifts across the input image.\n",
        "\n",
        "Purpose: Controls the density of the feature extraction. A larger stride skips pixels, resulting in a more \"compressed\" summary.\n",
        "\n",
        "Behavior: Stride 1 moves the filter one pixel at a time; Stride 2 jumps two pixels, reducing the output size significantly.\n",
        "\n",
        "**Effect on Output Dimensions**\n",
        "\n",
        "The output size ($O$) is calculated using: $O = \\frac{I - K + 2P}{S} + 1$(Where $I$=Input, $K$=Kernel/Filter size, $P$=Padding, $S$=Stride)\n",
        "\n",
        "Padding ($P$): Increasing padding increases the output dimension.\n",
        "\n",
        "Stride ($S$): Increasing stride decreases the output dimension (downsampling).\n",
        "\n",
        "Filter Size ($K$): Larger filters decrease output size unless compensated by padding."
      ],
      "metadata": {
        "id": "dQ5mj4iEQR4y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Define receptive field in the context of CNNs. Why is it important for deep\n",
        "architectures?\n",
        "\n",
        "Ans- **Definition**: The specific region of the input image that affects a particular neuron's output.\n",
        "\n",
        "**Growth**: In initial layers, the receptive field is small (local); in deeper layers, it becomes large (global).\n",
        "\n",
        "**Importance for Deep Architectures**\n",
        "\n",
        "Context: Enables the network to understand how small parts (e.g., eyes, nose) relate to form a whole object (e.g., a face).\n",
        "\n",
        "Hierarchy: Facilitates the transition from learning low-level features (edges) to high-level concepts (objects).\n",
        "\n",
        "Efficiency: Allows the model to capture global information using small, stacked filters rather than one massive, computationally expensive filter."
      ],
      "metadata": {
        "id": "V9VW8TA3Qwon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN.\n",
        "\n",
        "Ans- **Filter Size**\n",
        "\n",
        "Direct Impact: The number of parameters in a convolutional layer is directly proportional to the square of the filter size (e.g., a $3 \\times 3$ filter has 9 weights, while a $5 \\times 5$ has 25).\n",
        "\n",
        "Parameter Formula: Total parameters = $(K_w \\cdot K_h \\cdot C_{in} + 1) \\cdot C_{out}$ (where $K$ is filter dimensions and $C$ is channel count).\n",
        "\n",
        "Trade-off: Larger filters capture more spatial information but significantly increase the memory footprint and risk of overfitting.\n",
        "\n",
        "**Stride**\n",
        "\n",
        "No Direct Impact: Stride does not change the number of parameters in the convolutional layer itself; the filter weights remain the same regardless of how many pixels it skips.\n",
        "\n",
        "Indirect Impact: By increasing stride, the output feature map size decreases. This reduction significantly lowers the number of parameters in any subsequent Fully Connected (Dense) layers, as there are fewer flattened inputs to process."
      ],
      "metadata": {
        "id": "fc7E6lhZRDBK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance.\n",
        "\n",
        "Ans- **LeNet-5 (1998)**\n",
        "\n",
        "Depth: Very shallow (5 layers with learnable weights).\n",
        "\n",
        "Filter Sizes: Large filters (1$5 \\times 5$) relative to the input size (2$32 \\times 32$).\n",
        "\n",
        "Architecture: Uses Average Pooling and Sigmoid/Tanh activations.\n",
        "\n",
        "Performance: Designed for simple tasks like handwritten digit recognition (MNIST); lacks the capacity for complex natural images.\n",
        "\n",
        "**AlexNet (2012)**\n",
        "\n",
        "Depth: Medium (8 layers: 5 convolutional, 3 fully connected).\n",
        "\n",
        "Filter Sizes: Varied and large (starts with 5$11 \\times 11$, then 6$5 \\times 5$, and finally 7$3 \\times 3$).\n",
        "\n",
        "Architecture: Introduced ReLU activation and Dropout to prevent overfitting; used Max Pooling.\n",
        "\n",
        "Performance: Revolutionized deep learning by winning the ImageNet challenge, proving that deeper GPUs-based CNNs outperform traditional computer vision.\n",
        "\n",
        "**VGG-16 (2014)**\n",
        "\n",
        "Depth: Deep (16 layers).\n",
        "\n",
        "Filter Sizes: Consistent and small (12$3 \\times 3$ filters throughout).\n",
        "\n",
        "Architecture: Stacked multiple $3 \\times 3$ filters to simulate larger receptive fields with fewer parameters and more non-linearity (e.g., two $3 \\times 3$ layers behave like one $5 \\times 5$).\n",
        "\n",
        "Performance: Higher accuracy than AlexNet but very computationally expensive due to 138 million parameters."
      ],
      "metadata": {
        "id": "XFbRVGuTRWwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation."
      ],
      "metadata": {
        "id": "kNsXjhi5SBip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Load and Preprocess Data\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "# Reshape to include channel dimension (28x28x1) and normalize\n",
        "x_train = x_train.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "x_test = x_test.reshape(-1, 28, 28, 1).astype(\"float32\") / 255.0\n",
        "\n",
        "# 2. Build the CNN Model\n",
        "model = models.Sequential([\n",
        "    # Convolutional Layer 1: 32 filters, 3x3 kernel\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Convolutional Layer 2: 64 filters, 3x3 kernel\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "\n",
        "    # Flatten and Fully Connected Layers\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax') # 10 classes for digits 0-9\n",
        "])\n",
        "\n",
        "# 3. Compile the Model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# 4. Train the Model\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=64, validation_split=0.1)\n",
        "\n",
        "# 5. Evaluate the Model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f\"\\nTest Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_489vLI_SImh",
        "outputId": "f7278a34-f4c4-4c91-9297-0b6fd6180d34"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 49ms/step - accuracy: 0.8640 - loss: 0.4437 - val_accuracy: 0.9793 - val_loss: 0.0734\n",
            "Epoch 2/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 50ms/step - accuracy: 0.9790 - loss: 0.0658 - val_accuracy: 0.9852 - val_loss: 0.0509\n",
            "Epoch 3/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 49ms/step - accuracy: 0.9867 - loss: 0.0427 - val_accuracy: 0.9877 - val_loss: 0.0443\n",
            "Epoch 4/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 47ms/step - accuracy: 0.9904 - loss: 0.0299 - val_accuracy: 0.9905 - val_loss: 0.0353\n",
            "Epoch 5/5\n",
            "\u001b[1m844/844\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 48ms/step - accuracy: 0.9927 - loss: 0.0228 - val_accuracy: 0.9885 - val_loss: 0.0390\n",
            "\n",
            "Test Accuracy: 0.9899\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture."
      ],
      "metadata": {
        "id": "TG4nEDPXUmvS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. PreprocessingNormalization:\n",
        "\n",
        "Pixels are scaled from $[0, 255]$ to $[0, 1]$.\n",
        "\n",
        "One-Hot Encoding: Labels are converted from integers to binary class matrices."
      ],
      "metadata": {
        "id": "y3X0N9DhUw7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, utils\n",
        "\n",
        "# Load dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoding\n",
        "y_train = utils.to_categorical(y_train, 10)\n",
        "y_test = utils.to_categorical(y_test, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9Z0fx5vUomx",
        "outputId": "3ab5b183-c433-4bc1-b7f8-87a1e431be2f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. CNN Architecture\n",
        "Since RGB images are more complex than grayscale digits, we use a deeper architecture with Dropout and Batch Normalization to ensure stability."
      ],
      "metadata": {
        "id": "r7TeK2OTU3vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential([\n",
        "    # Block 1\n",
        "    layers.Conv2D(32, (3, 3), padding='same', activation='relu', input_shape=(32, 32, 3)),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(32, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.2),\n",
        "\n",
        "    # Block 2\n",
        "    layers.Conv2D(64, (3, 3), padding='same', activation='relu'),\n",
        "    layers.BatchNormalization(),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D((2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    # Classifier\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkBAFXIJU56T",
        "outputId": "14dc3563-d007-44fc-a63a-6c8c24efd669"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training & Evaluation"
      ],
      "metadata": {
        "id": "rY8hA1Z_U-2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train, epochs=10, batch_size=64, validation_split=0.2)\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CYeHMkNxU_eb",
        "outputId": "2e4adbb3-3d38-4b02-b400-07a17bf54d49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 312ms/step - accuracy: 0.2326 - loss: 2.1194 - val_accuracy: 0.4375 - val_loss: 1.5568\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 325ms/step - accuracy: 0.4377 - loss: 1.5444 - val_accuracy: 0.5226 - val_loss: 1.3531\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m187s\u001b[0m 299ms/step - accuracy: 0.5484 - loss: 1.2505 - val_accuracy: 0.5796 - val_loss: 1.2859\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m185s\u001b[0m 296ms/step - accuracy: 0.6148 - loss: 1.0961 - val_accuracy: 0.6541 - val_loss: 0.9611\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m188s\u001b[0m 301ms/step - accuracy: 0.6484 - loss: 0.9956 - val_accuracy: 0.6559 - val_loss: 0.9358\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m196s\u001b[0m 292ms/step - accuracy: 0.6756 - loss: 0.9265 - val_accuracy: 0.6954 - val_loss: 0.8753\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m184s\u001b[0m 295ms/step - accuracy: 0.7017 - loss: 0.8651 - val_accuracy: 0.6922 - val_loss: 0.8849\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 319ms/step - accuracy: 0.7118 - loss: 0.8241 - val_accuracy: 0.7374 - val_loss: 0.7525\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m217s\u001b[0m 343ms/step - accuracy: 0.7276 - loss: 0.7872 - val_accuracy: 0.7173 - val_loss: 0.8077\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m192s\u001b[0m 307ms/step - accuracy: 0.7296 - loss: 0.7636 - val_accuracy: 0.7474 - val_loss: 0.7347\n",
            "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 35ms/step - accuracy: 0.7510 - loss: 0.7374\n",
            "Test Accuracy: 0.7469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation."
      ],
      "metadata": {
        "id": "Axai6CBRTX4L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Model Definition\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=3, padding=1), # Input: 1x28x28\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2),                         # Output: 32x14x14\n",
        "            nn.Conv2d(32, 64, kernel_size=3),           # Output: 64x12x12\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2, 2)                          # Output: 64x6x6\n",
        "        )\n",
        "        self.fc_layers = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 6 * 6, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc_layers(self.conv_layers(x))\n",
        "\n",
        "# 2. Data Loaders\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
        "train_loader = DataLoader(datasets.MNIST('./data', train=True, download=True, transform=transform), batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(datasets.MNIST('./data', train=False, transform=transform), batch_size=1000)\n",
        "\n",
        "# 3. Initialization\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 4. Training Loop\n",
        "model.train()\n",
        "for epoch in range(2): # 2 Epochs for brevity\n",
        "    for data, target in train_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch+1} complete.\")\n",
        "\n",
        "# 5. Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        output = model(data)\n",
        "        pred = output.argmax(dim=1, keepdim=True)\n",
        "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "print(f\"\\nTest Accuracy: {100. * correct / len(test_loader.dataset):.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ebr0b6_Tkf8",
        "outputId": "52123bee-dcb9-43e9-9219-92d3b09982e8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.91M/9.91M [00:00<00:00, 12.9MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.9k/28.9k [00:00<00:00, 343kB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.65M/1.65M [00:00<00:00, 3.19MB/s]\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.54k/4.54k [00:00<00:00, 9.60MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 complete.\n",
            "Epoch 2 complete.\n",
            "\n",
            "Test Accuracy: 98.68%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model."
      ],
      "metadata": {
        "id": "sAvgo518VKzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define paths\n",
        "base_dir = 'data'\n",
        "sub_dirs = ['train/class_A', 'train/class_B', 'val/class_A', 'val/class_B']\n",
        "\n",
        "# 2. Create folders\n",
        "for sub_dir in sub_dirs:\n",
        "    os.makedirs(os.path.join(base_dir, sub_dir), exist_ok=True)\n",
        "\n",
        "# 3. Generate dummy images\n",
        "def create_dummy_images(path, count, color):\n",
        "    for i in range(count):\n",
        "        # Create a 150x150 image with a specific color\n",
        "        img = np.full((150, 150, 3), color, dtype=np.uint8)\n",
        "        # Add random noise/shapes so filters have something to detect\n",
        "        cv2.putText(img, str(i), (50, 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n",
        "        cv2.imwrite(os.path.join(path, f'img_{i}.jpg'), img)\n",
        "\n",
        "# Create 100 training and 20 validation images per class\n",
        "create_dummy_images('data/train/class_A', 100, [255, 0, 0])  # Blue images\n",
        "create_dummy_images('data/train/class_B', 100, [0, 0, 255])  # Red images\n",
        "create_dummy_images('data/val/class_A', 20, [255, 0, 0])\n",
        "create_dummy_images('data/val/class_B', 20, [0, 0, 255])\n",
        "\n",
        "print(\"Dataset generated successfully in /data folder.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AG4pNPSNeW2b",
        "outputId": "5faaea95-91f6-4285-925f-049774bee55a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset generated successfully in /data folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# 1. Preprocessing & Augmentation\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,           # Normalize pixel values\n",
        "    rotation_range=20,        # Data augmentation\n",
        "    width_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255) # Only rescale for validation\n",
        "\n",
        "# 2. Load Images from Directory\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'data/train',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'       # Use 'categorical' for >2 classes\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    'data/val',\n",
        "    target_size=(150, 150),\n",
        "    batch_size=32,\n",
        "    class_mode='binary'\n",
        ")\n",
        "\n",
        "# 3. CNN Architecture\n",
        "model = models.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(2, 2),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(64, activation='relu'),\n",
        "    layers.Dense(1, activation='sigmoid') # Binary output\n",
        "])\n",
        "\n",
        "# 4. Compile and Train\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvPkEMmqVNx8",
        "outputId": "12f2ce54-fc45-48aa-c529-31e42b1acee4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 200 images belonging to 2 classes.\n",
            "Found 40 images belonging to 2 classes.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 1s/step - accuracy: 0.7810 - loss: 0.3252 - val_accuracy: 1.0000 - val_loss: 7.6688e-08\n",
            "Epoch 2/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 5.4697e-08 - val_accuracy: 1.0000 - val_loss: 4.4956e-10\n",
            "Epoch 3/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 970ms/step - accuracy: 1.0000 - loss: 8.2347e-10 - val_accuracy: 1.0000 - val_loss: 2.8549e-11\n",
            "Epoch 4/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 8.7051e-11 - val_accuracy: 1.0000 - val_loss: 8.6918e-12\n",
            "Epoch 5/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 939ms/step - accuracy: 1.0000 - loss: 2.6848e-11 - val_accuracy: 1.0000 - val_loss: 4.7364e-12\n",
            "Epoch 6/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 2.0879e-11 - val_accuracy: 1.0000 - val_loss: 3.5346e-12\n",
            "Epoch 7/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.6656e-11 - val_accuracy: 1.0000 - val_loss: 3.0657e-12\n",
            "Epoch 8/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.7395e-11 - val_accuracy: 1.0000 - val_loss: 2.8510e-12\n",
            "Epoch 9/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.4058e-11 - val_accuracy: 1.0000 - val_loss: 2.7477e-12\n",
            "Epoch 10/10\n",
            "\u001b[1m7/7\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 1s/step - accuracy: 1.0000 - loss: 1.1043e-11 - val_accuracy: 1.0000 - val_loss: 2.6968e-12\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7c7e9c665c40>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into â€œNormalâ€ and â€œPneumoniaâ€ categories. Describe your end-to-end approachâ€“from data preparation and model training to deploying the model as a web app using Streamlit."
      ],
      "metadata": {
        "id": "EoN3r6EXUaB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Data Preparation & Preprocessing**\n",
        "\n",
        "Data Source: Use the NIH or Kaggle Chest X-ray dataset.\n",
        "\n",
        "Preprocessing: Resize images to a standard resolution (e.g., $224 \\times 224$). Convert grayscale X-rays to 3-channel (RGB) to leverage pre-trained weights.\n",
        "\n",
        "Augmentation: Apply subtle rotations and horizontal flips to account for varying patient positioning, but avoid vertical flips which are medically unrealistic.\n",
        "\n",
        "Normalization: Subtract the mean and divide by the standard deviation of the dataset to stabilize training.\n",
        "\n",
        "2. **Model Architecture**\n",
        "\n",
        "Transfer Learning: Use ResNet50 or EfficientNet pre-trained on ImageNet. These models already understand textures and edges, requiring only fine-tuning.\n",
        "\n",
        "Custom Head: Replace the final layers with a Global Average Pooling layer followed by a Dense layer ($1$ unit) with a Sigmoid activation.\n",
        "\n",
        "Optimization: Use the Adam optimizer and Binary Cross-Entropy loss.\n",
        "\n",
        "**3. Training & Evaluation**\n",
        "\n",
        "Class Imbalance: Medical datasets often have fewer pneumonia cases. Use class weights or SMOTE to balance the influence during training.\n",
        "\n",
        "Metrics: Prioritize Recall (Sensitivity) to minimize False Negatives (missing a sick patient) and use the AUC-ROC curve to evaluate threshold performance.\n",
        "\n",
        "Interpretability: Implement Grad-CAM to generate heatmaps, showing the radiologist which areas of the lung the CNN used to make its prediction.\n",
        "\n",
        "**4. Deployment with Streamlit**"
      ],
      "metadata": {
        "id": "pCQOsNAdV_Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pillow numpy tensorflow -q\n",
        "!npm install -g localtunnel -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jI2TnmUcfoM3",
        "outputId": "8a78635c-1a54-4e0e-c175-697f485f0e38"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "changed 22 packages in 2s\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K3 packages are looking for funding\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K  run `npm fund` for details\n",
            "\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0K"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import tensorflow as tf\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# 1. Load Model (with caching to prevent lag)\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    if os.path.exists('pneumonia_model.h5'):\n",
        "        return tf.keras.models.load_model('pneumonia_model.h5')\n",
        "    # Fallback dummy model for testing UI\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Input(shape=(224, 224, 3)),\n",
        "        tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model = load_model()\n",
        "\n",
        "# 2. UI Layout\n",
        "st.set_page_config(page_title=\"Medical AI\", layout=\"centered\")\n",
        "st.title(\"ğŸ« Chest X-Ray Pneumonia Classifier\")\n",
        "st.write(\"Upload a patient's chest X-ray to analyze for Pneumonia.\")\n",
        "\n",
        "uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "\n",
        "if uploaded_file is not None:\n",
        "    # Display Image\n",
        "    image = Image.open(uploaded_file).convert('RGB')\n",
        "    st.image(image, caption='Uploaded X-ray scan', use_container_width=True)\n",
        "\n",
        "    # Preprocess\n",
        "    img = image.resize((224, 224))\n",
        "    img_array = np.expand_dims(np.array(img) / 255.0, axis=0)\n",
        "\n",
        "    # Predict\n",
        "    with st.spinner('Analyzing scan...'):\n",
        "        prediction = model.predict(img_array, verbose=0)\n",
        "        label = \"PNEUMONIA\" if prediction[0][0] > 0.5 else \"NORMAL\"\n",
        "        confidence = prediction[0][0] if label == \"PNEUMONIA\" else 1 - prediction[0][0]\n",
        "\n",
        "    # Results\n",
        "    color = \"red\" if label == \"PNEUMONIA\" else \"green\"\n",
        "    st.subheader(f\"Prediction: :{color}[{label}]\")\n",
        "    st.write(f\"**Confidence Level:** {confidence:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fRPjsrTkV_f",
        "outputId": "dd7cfb4d-8916-45fa-c47e-4ba809f8a62c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step A: Get your IP (This is your password for the link)\n",
        "print(\"1. Copy this IP address (Password):\")\n",
        "!curl ipv4.icanhazip.com\n",
        "\n",
        "# Step B: Start Streamlit and Tunnel\n",
        "print(\"\\n2. Click the link below once it appears:\")\n",
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_0G5vy_kYaY",
        "outputId": "53014650-7424-4219-c665-87a14378c831"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Copy this IP address (Password):\n",
            "34.6.109.97\n",
            "\n",
            "2. Click the link below once it appears:\n",
            "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0Kâ ‡\u001b[1G\u001b[0Kâ \u001b[1G\u001b[0Kâ ‹\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kyour url is: https://chatty-ducks-hide.loca.lt\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8502\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.6.109.97:8502\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Production Considerations**\n",
        "\n",
        "CI/CD: Use GitHub Actions to automate model testing.\n",
        "\n",
        "API Layer: For high traffic, wrap the model in FastAPI and containerize it using Docker before deploying to AWS or Google Cloud."
      ],
      "metadata": {
        "id": "GngPyWCnWgC7"
      }
    }
  ]
}