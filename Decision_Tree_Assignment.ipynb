{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is a Decision Tree, and how does it work in the context of\n",
        "classification?\n",
        "\n",
        "Ans- A tree-like model used to make decisions by splitting data into branches based on feature values.\n",
        "\n",
        "**How It Works**\n",
        "Root Node: Starts with the entire dataset.\n",
        "\n",
        "Splitting: Asks a \"Yes/No\" question about a specific feature (e.g., \"Is Age > 30?\").\n",
        "\n",
        "Branches: Based on the answer, the data moves down different paths.\n",
        "\n",
        "Leaf Nodes: The final points where a class label is assigned (e.g., \"Will Buy\" or \"Won't Buy\").\n",
        "\n",
        "**The Goal**\n",
        "The tree uses metrics like Gini Impurity or Information Gain to find the questions that best separate the classes into \"pure\" groups."
      ],
      "metadata": {
        "id": "RBAbN22g_P8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the concepts of Gini Impurity and Entropy as impurity measures.\n",
        "How do they impact the splits in a Decision Tree?\n",
        "\n",
        "Ans- **Gini Impurity**\n",
        "Measures the probability of a random element being incorrectly classified if it were labeled according to the distribution in the node.\n",
        "\n",
        "Formula: $G = 1 - \\sum (p_i)^2$\n",
        "\n",
        "Range: 0 (Pure) to 0.5 (Balanced/Max Impurity).\n",
        "\n",
        "Characteristic: Favors the largest class and is computationally faster because it doesn't use logarithms.\n",
        "\n",
        "**Entropy** Measures the \"disorder\" or unpredictability of the data.\n",
        "\n",
        "Formula: $H = -\\sum p_i \\log_2(p_i)$\n",
        "\n",
        "Range: 0 (Pure) to 1.0 (Balanced/Max Impurity).\n",
        "\n",
        "Characteristic: Used to calculate Information Gain. It is more computationally expensive than Gini due to the log calculation.\n",
        "\n",
        "**Impact on Splits**\n",
        "\n",
        "**Selection**: The tree tests every possible split across all features.\n",
        "\n",
        "Comparison: It calculates the impurity before and after the split.\n",
        "\n",
        "Optimization: It chooses the split that results in the greatest reduction in impurity (the \"purest\" child nodes).8In short: Lower impurity equals a better split."
      ],
      "metadata": {
        "id": "nEIMCYv0_isy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.  What is the difference between Pre-Pruning and Post-Pruning in Decision\n",
        "Trees? Give one practical advantage of using each\n",
        "\n",
        "Ans- **Pre-Pruning (Early Stopping)**\n",
        "\n",
        "Mechanism: Stops the tree-building process before it becomes too complex. It uses parameters like max_depth, min_samples_split, or min_samples_leaf.\n",
        "\n",
        "\n",
        "Advantage: Efficiency. It saves significant time and memory by preventing the tree from growing unnecessary branches in the first place.\n",
        "\n",
        "**Post-Pruning (Cost Complexity Pruning)**\n",
        "\n",
        "Mechanism: Allows the tree to grow to its full size (where it likely overfits) and then removes branches that provide little predictive power.\n",
        "\n",
        "Advantage: Better Performance. It allows the model to capture complex relationships that might be missed by early stopping, only removing them if they prove to be \"noise\" during the pruning phase."
      ],
      "metadata": {
        "id": "fGi_oRNMAcms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is Information Gain in Decision Trees, and why is it important for\n",
        "choosing the best split?\n",
        "\n",
        "Ans- The reduction in Entropy (disorder) achieved by partitioning a dataset based on a specific attribute.\n",
        "\n",
        "**How it works**\n",
        "\n",
        "Initial Entropy: Measure the uncertainty of the parent node.\n",
        "\n",
        "Weighted Entropy: Calculate the average entropy of the resulting child nodes after a split.\n",
        "\n",
        "Subtraction: $\\text{Information Gain} = \\text{Entropy(Parent)} - \\text{Weighted Entropy(Children)}$.\n",
        "\n",
        "**Importance for Choosing Splits**\n",
        "\n",
        "Optimization: It acts as the selection criterion. The algorithm calculates Information Gain for every possible split and chooses the one with the highest value.\n",
        "\n",
        "Purity: High Information Gain ensures that the resulting child nodes are as \"pure\" as possible (containing mostly one class), making the classification more accurate."
      ],
      "metadata": {
        "id": "RJkzCbgsAtXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What are some common real-world applications of Decision Trees, and\n",
        "what are their main advantages and limitations?\n",
        "\n",
        "Ans- **Real-World Applications**\n",
        "\n",
        "Finance: Credit scoring and loan default prediction.\n",
        "\n",
        "Healthcare: Diagnosing diseases based on patient symptoms.\n",
        "\n",
        "Marketing: Predicting customer churn or response to a campaign.\n",
        "\n",
        "**Advantages**\n",
        "\n",
        "Interpretability: They are easy to visualize and explain to non-technical users.\n",
        "\n",
        "No Preprocessing: Requires little data scaling or normalization.\n",
        "\n",
        "Feature Importance: Automatically identifies which variables are most significant.\n",
        "\n",
        "**Limitations**\n",
        "\n",
        "Overfitting: Trees can become overly complex and fail to generalize to new data.\n",
        "\n",
        "Instability: Small changes in the data can result in a completely different tree structure.\n",
        "\n",
        "Bias: They can be biased toward features with more levels or categories."
      ],
      "metadata": {
        "id": "D1lZlPoSBIBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Train a Decision Tree Classifier using the Gini criterion\n",
        "\n",
        "Print the model’s accuracy and feature importances"
      ],
      "metadata": {
        "id": "0xq1IIG3BYlK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train model using Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and Accuracy\n",
        "y_pred = clf.predict(X_test)\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Feature Importances\n",
        "for name, importance in zip(iris.feature_names, clf.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xi1xVZuBjkK",
        "outputId": "e944434c-1fa1-46df-dc30-60fb2006345b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.00\n",
            "sepal length (cm): 0.0000\n",
            "sepal width (cm): 0.0191\n",
            "petal length (cm): 0.8933\n",
            "petal width (cm): 0.0876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to\n",
        "a fully-grown tree."
      ],
      "metadata": {
        "id": "dep_DmvnBp_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 1. Pruned Tree (max_depth=3)\n",
        "pruned_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "pruned_tree.fit(X_train, y_train)\n",
        "acc_pruned = accuracy_score(y_test, pruned_tree.predict(X_test))\n",
        "\n",
        "# 2. Fully-Grown Tree (No depth limit)\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "acc_full = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "print(f\"Accuracy (max_depth=3): {acc_pruned:.4f}\")\n",
        "print(f\"Accuracy (Fully Grown): {acc_full:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcpkdOknB0Hi",
        "outputId": "d6495da1-9eb1-4266-97ba-ee128380764e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy (max_depth=3): 1.0000\n",
            "Accuracy (Fully Grown): 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparison Summary**\n",
        "\n",
        "Fully-Grown Tree: High risk of overfitting, capturing noise as rules.\n",
        "\n",
        "Pruned Tree (max_depth=3): Generally more robust, simpler to interpret, and often generalizes better to new data despite potentially lower training accuracy."
      ],
      "metadata": {
        "id": "PSHY6ylPB7v6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to:\n",
        "\n",
        "Load the Boston Housing Dataset\n",
        "\n",
        "Train a Decision Tree Regressor\n",
        "\n",
        "Print the Mean Squared Error (MSE) and feature importances"
      ],
      "metadata": {
        "id": "MaJLh_SJCBTi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Generate local synthetic data (100 samples, 5 features)\n",
        "X, y = make_regression(n_samples=100, n_features=5, noise=0.1, random_state=42)\n",
        "feature_names = [f\"Feature {i}\" for i in range(5)]\n",
        "\n",
        "# 2. Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Train Decision Tree Regressor\n",
        "regressor = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
        "regressor.fit(X_train, y_train)\n",
        "\n",
        "# 4. Predict and calculate MSE\n",
        "y_pred = regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "print(f\"Mean Squared Error: {mse:.4f}\")\n",
        "print(\"\\nFeature Importances:\")\n",
        "for name, importance in zip(feature_names, regressor.feature_importances_):\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xYjBNWIPDMgi",
        "outputId": "9971085d-ad62-4e80-c5c2-6428a0803f92"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 7166.5039\n",
            "\n",
            "Feature Importances:\n",
            "Feature 0: 0.2276\n",
            "Feature 1: 0.6522\n",
            "Feature 2: 0.0000\n",
            "Feature 3: 0.1203\n",
            "Feature 4: 0.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to:\n",
        "\n",
        "Load the Iris Dataset\n",
        "\n",
        "Tune the Decision Tree’s max_depth and min_samples_split using\n",
        "GridSearchCV\n",
        "\n",
        "Print the best parameters and the resulting model accuracy"
      ],
      "metadata": {
        "id": "NbHBXM0YDReq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Define parameters to tune\n",
        "param_grid = {\n",
        "    'max_depth': [2, 3, 4, 5, 6],\n",
        "    'min_samples_split': [2, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best parameters and accuracy\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Accuracy with Best Model: {accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4No5TleDXJy",
        "outputId": "56766e29-2fab-4109-ca4f-507ebfd47902"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 4, 'min_samples_split': 10}\n",
            "Accuracy with Best Model: 1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.  Imagine you’re working as a data scientist for a healthcare company that\n",
        "wants to predict whether a patient has a certain disease. You have a large dataset with mixed data types and some missing values.\n",
        "\n",
        "Explain the step-by-step process you would follow to:\n",
        "\n",
        "Handle the missing values\n",
        "\n",
        "Encode the categorical features\n",
        "\n",
        "Train a Decision Tree model\n",
        "\n",
        "Tune its hyperparameters\n",
        "\n",
        "Evaluate its performance\n",
        "And describe what business value this model could provide in the real-world\n",
        "setting.\n",
        "\n",
        "Ans- 1. **Handle Missing Values**\n",
        "\n",
        "Identify: Check if data is missing randomly or systematically.\n",
        "\n",
        "Imputation: For numerical values (e.g., blood pressure), use Median Imputation to avoid outlier bias. For categorical values (e.g., smoking status), use Mode Imputation or create a new \"Missing\" category.\n",
        "\n",
        "2. **Encode Categorical Features**\n",
        "\n",
        "Binary/Ordinal: Use Label Encoding for features with logical order (e.g., Stage 1, 2, 3).\n",
        "\n",
        "Nominal: Use One-Hot Encoding for features without order (e.g., Blood Type) to ensure the model doesn't assume a false mathematical relationship between categories.\n",
        "\n",
        "3. **Train a Decision Tree Model**\n",
        "\n",
        "Split: Divide data into training (80%) and testing (20%) sets.\n",
        "\n",
        "Fit: Initialize DecisionTreeClassifier and fit it to the training data. Decision trees are excellent for mixed data types as they don't require feature scaling (like normalization).\n",
        "\n",
        "4. **Tune Hyperparameters**\n",
        "\n",
        "Grid Search: Use GridSearchCV to test combinations of max_depth (to prevent overfitting) and min_samples_leaf (to ensure nodes are statistically significant).\n",
        "\n",
        "5. **Evaluate Performance**\n",
        "\n",
        "Recall (Sensitivity): Crucial in healthcare. We must minimize False Negatives (missing a sick patient).\n",
        "\n",
        "F1-Score: To balance Precision and Recall.\n",
        "\n",
        "ROC-AUC: To evaluate how well the model distinguishes between healthy and diseased patients.\n",
        "\n",
        "**Business Value**\n",
        "\n",
        "Early Intervention: Identifies high-risk patients sooner, improving recovery rates and saving lives.\n",
        "\n",
        "Resource Allocation: Helps hospitals prioritize urgent cases and manage staff more efficiently.\n",
        "\n",
        "Cost Reduction: Prevents expensive late-stage treatments through proactive screening."
      ],
      "metadata": {
        "id": "-cpfto9ODiNi"
      }
    }
  ]
}